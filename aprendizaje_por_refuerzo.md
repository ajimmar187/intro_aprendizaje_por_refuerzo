

# Aprendizaje por Refuerzo: Fundamentos y Aplicaciones Pr√°cticas

## 1. Introducci√≥n

En el cap√≠tulo anterior exploramos modelos multimodales para el aprendizaje conjunto de im√°genes y texto. Ahora, en este √∫ltimo cap√≠tulo, abordaremos el aprendizaje por refuerzo (RL), que representa la tercera categor√≠a fundamental de tareas de aprendizaje autom√°tico que mencionamos al principio del libro.

El aprendizaje por refuerzo se distingue del aprendizaje supervisado y no supervisado en que se centra en aprender a trav√©s de la experiencia e interacci√≥n directa con el entorno. Este paradigma permite a los agentes desarrollar comportamientos √≥ptimos mediante la retroalimentaci√≥n recibida de sus acciones.

### 1.1 Contenido del cap√≠tulo

1. **Fundamentos de aprendizaje por refuerzo**
   * Conceptos clave y terminolog√≠a
   * Formulaci√≥n matem√°tica del problema

2. **Entornos de desarrollo (Gymnasium)**
   * Instalaci√≥n y configuraci√≥n
   * Caracter√≠sticas principales

3. **M√©todos de soluci√≥n**
   * Programaci√≥n din√°mica en el entorno FrozenLake
   * M√©todos de Monte Carlo en Blackjack
   * Aprendizaje por diferencias temporales (Q-learning)

## 2. Herramientas de Desarrollo: Gymnasium

### 2.1 Origen y evoluci√≥n

[Gymnasium](https://gymnasium.farama.org/) es el sucesor oficial de OpenAI Gym, una biblioteca para el desarrollo y comparaci√≥n de algoritmos de aprendizaje por refuerzo. Tras la decisi√≥n de OpenAI de discontinuar el mantenimiento de Gym, la Fundaci√≥n Farama asumi√≥ el desarrollo y cre√≥ Gymnasium como su sustituto directo, preservando la compatibilidad con las versiones anteriores mientras a√±ade mejoras significativas.

### 2.2 Caracter√≠sticas principales

Gymnasium ofrece una serie de ventajas fundamentales para el desarrollo de algoritmos de RL:

* **Interfaz unificada**: API consistente para interactuar con diversos entornos
* **Diversidad de entornos**: Desde problemas cl√°sicos hasta simulaciones complejas
* **Facilidad de uso**: Dise√±ado para ser accesible para principiantes y expertos
* **Reproducibilidad**: Facilita la comparaci√≥n objetiva entre algoritmos
* **Comunidad activa**: Constante evoluci√≥n y mejoras

### 2.3 Instalaci√≥n

Para instalar Gymnasium, podemos utilizar el gestor de paquetes pip:

```bash
# Instalaci√≥n b√°sica
pip install gymnasium

# Con entornos adicionales (recomendado para este cap√≠tulo)
pip install gymnasium[toy-text]
```

El m√≥dulo `toy-text` incluye entornos cl√°sicos como FrozenLake y Blackjack que utilizaremos en nuestros ejemplos.

## 3. Fundamentos del Aprendizaje por Refuerzo

### 3.1 Conceptualizaci√≥n mediante ejemplos

El aprendizaje por refuerzo puede entenderse mediante analog√≠as con situaciones cotidianas. Pensemos en los videojuegos cl√°sicos como Super Mario Bros o Sonic the Hedgehog: el jugador (agente) debe navegar por niveles (entorno), tomando decisiones (acciones) como saltar o correr, bas√°ndose en lo que ve en pantalla (estados), con el objetivo de maximizar la puntuaci√≥n (recompensa).

El proceso fundamental del aprendizaje por refuerzo sigue este ciclo:

1. Observaci√≥n del estado actual
2. Selecci√≥n y ejecuci√≥n de una acci√≥n
3. Recepci√≥n de una recompensa y transici√≥n a un nuevo estado
4. Repetici√≥n hasta alcanzar un estado terminal

### 3.2 Componentes esenciales

Los elementos b√°sicos que componen cualquier problema de aprendizaje por refuerzo son:

* **Entorno**: El mundo con el que interact√∫a el agente
  * *Ejemplos*: Tablero de ajedrez, simulaci√≥n de carreteras, mundo virtual

* **Agente**: Entidad que toma decisiones
  * *Ejemplos*: Personaje en un videojuego, veh√≠culo aut√≥nomo, robot

* **Estados**: Representaci√≥n de la situaci√≥n actual
  * *Ejemplos*: Posici√≥n en un tablero, velocidad y coordenadas de un veh√≠culo

* **Acciones**: Decisiones disponibles para el agente
  * *Ejemplos*: Movimientos en juegos, aceleraci√≥n/frenado en veh√≠culos

* **Recompensas**: Feedback num√©rico que gu√≠a el aprendizaje
  * *Ejemplos*: +1 por capturar una pieza, -10 por colisionar

### 3.3 Recompensas acumuladas y factor de descuento

El objetivo del agente es maximizar la recompensa total futura (tambi√©n llamada "retorno"). Sin embargo, no todas las recompensas futuras tienen el mismo valor presente.

Para modelar esta realidad, se introduce el **factor de descuento** (Œ≥), un valor entre 0 y 1 que determina cu√°nto "importan" las recompensas futuras:

* Œ≥ cercano a 0: Preferencia por recompensas inmediatas
* Œ≥ cercano a 1: Valoraci√≥n similar entre recompensas presentes y futuras

Matem√°ticamente, el retorno descontado Gt en el tiempo t se define como:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Donde:
- $G_t$ es el retorno descontado en el tiempo t
- $R_{t+k+1}$ es la recompensa recibida k+1 pasos despu√©s del tiempo t
- $\gamma$ es el factor de descuento (0 ‚â§ Œ≥ ‚â§ 1)

### 3.4 Enfoques principales

Existen dos grandes categor√≠as de m√©todos para resolver problemas de aprendizaje por refuerzo:

#### 3.4.1 Enfoque basado en pol√≠tica

Una **pol√≠tica** (œÄ) es una funci√≥n que mapea estados a acciones, indicando qu√© acci√≥n debe tomar el agente en cada estado. Las pol√≠ticas pueden ser:

* **Deterministas**: Para cada estado existe una √∫nica acci√≥n definida
* **Estoc√°sticas**: Asignan probabilidades a diferentes acciones en cada estado

Los m√©todos basados en pol√≠tica aprenden directamente esta funci√≥n de mapeo.

**Ejemplo**: Un conductor de F√≥rmula 1 aprende exactamente qu√© maniobra realizar en cada punto y condici√≥n de la pista.

#### 3.4.2 Enfoque basado en valor

Este enfoque se centra en aprender la funci√≥n de valor, que estima la utilidad esperada (recompensa futura descontada) de estar en un estado particular o de realizar una acci√≥n espec√≠fica en un estado determinado:

* **Funci√≥n de valor de estado V(s)**: Valor esperado de estar en el estado s
* **Funci√≥n de valor de acci√≥n Q(s,a)**: Valor esperado de tomar la acci√≥n a en el estado s

Las decisiones se toman eligiendo acciones que conducen a estados con mayor valor.

**Ejemplo**: Un buscador de tesoros que sigue el camino que promete la mayor recompensa, sin conocer necesariamente todas las decisiones futuras.

## 4. Resolviendo el Entorno FrozenLake con Programaci√≥n Din√°mica

### 4.1 Caracter√≠sticas del entorno FrozenLake

FrozenLake es un entorno discreto cl√°sico donde un agente debe navegar por un lago congelado representado como una cuadr√≠cula. El objetivo es moverse desde el punto de inicio hasta la meta sin caer en agujeros.

![FrozenLake Ejemplo](https://gymnasium.farama.org/_images/frozen_lake.gif)

Las caracter√≠sticas principales del entorno son:

* **Tama√±o**: Disponible en versiones 4x4 y 8x8
* **Tipos de casillas**:
  - Inicio (S): Punto de partida
  - Meta (G): Objetivo, otorga recompensa +1
  - Hielo (F): Casilla segura de tr√°nsito
  - Agujero (H): Termina el episodio sin recompensa

* **Acciones posibles**:
  - 0: Izquierda
  - 1: Abajo
  - 2: Derecha
  - 3: Arriba

* **Din√°mica estoc√°stica**: El hielo es resbaladizo, lo que significa que el agente puede "deslizarse" a una casilla diferente con cierta probabilidad.

### 4.2 Creaci√≥n y simulaci√≥n del entorno

Para comenzar a trabajar con FrozenLake, primero creamos una instancia del entorno:

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import torch

# Crear el entorno
env = gym.make("FrozenLake-v1", render_mode="rgb_array")

# Obtener dimensiones del espacio de estados y acciones
n_states = env.observation_space.n  # 16 estados para 4x4
n_actions = env.action_space.n      # 4 acciones posibles
```

Para reiniciar y visualizar el entorno:

```python
# Reiniciar el entorno
state, info = env.reset()

# Visualizar el estado actual
plt.figure(figsize=(4, 4))
plt.imshow(env.render())
plt.title(f"Estado actual: {state}")
plt.axis('off')
plt.show()
```

Ejecutar una acci√≥n y observar el resultado:

```python
# Ejecutar acci√≥n (ejemplo: mover a la derecha)
next_state, reward, terminated, truncated, info = env.step(2)

# Visualizar el nuevo estado
plt.figure(figsize=(4, 4))
plt.imshow(env.render())
plt.title(f"Nuevo estado: {next_state}, Recompensa: {reward}")
plt.axis('off')
plt.show()
```

### 4.3 Evaluaci√≥n de pol√≠ticas aleatorias

Antes de implementar algoritmos avanzados, evaluemos el rendimiento de pol√≠ticas aleatorias:

```python
def run_episode(env, policy):
    """Ejecuta un episodio completo siguiendo una pol√≠tica dada."""
    state, _ = env.reset()
    total_reward = 0
    done = False
    
    while not done:
        # Seleccionar acci√≥n seg√∫n la pol√≠tica
        action = int(policy[state].item())
        
        # Ejecutar acci√≥n
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        # Acumular recompensa
        total_reward += reward
        
    return total_reward

# Generar y evaluar pol√≠ticas aleatorias
n_episodes = 1000
rewards = []

for _ in range(n_episodes):
    # Crear pol√≠tica aleatoria (una acci√≥n por estado)
    random_policy = torch.randint(high=n_actions, size=(n_states,))
    
    # Ejecutar un episodio
    reward = run_episode(env, random_policy)
    rewards.append(reward)

# Calcular rendimiento promedio
avg_reward = sum(rewards) / n_episodes
print(f"Recompensa promedio con pol√≠ticas aleatorias: {avg_reward:.4f}")
```

El resultado t√≠pico de este experimento es aproximadamente 0.016, lo que significa que solo el 1.6% de los episodios termina exitosamente con pol√≠ticas aleatorias. Esto demuestra la dificultad del entorno a pesar de su aparente simplicidad.

### 4.4 An√°lisis de la matriz de transici√≥n

Una manera de comprender mejor el comportamiento estoc√°stico del entorno es examinar su matriz de transici√≥n:

```python
# Examinar la matriz de transici√≥n para un estado espec√≠fico
state_to_examine = 6
print("Matriz de transici√≥n para el estado 6:")
for action in range(n_actions):
    print(f"Acci√≥n {action}:")
    for trans_prob, next_state, reward, is_terminal in env.env.P[state_to_examine][action]:
        print(f"  Prob: {trans_prob}, Estado siguiente: {next_state}, " 
              f"Recompensa: {reward}, Terminal: {is_terminal}")
```

La salida muestra la probabilidad de transici√≥n a diferentes estados, dependiendo de la acci√≥n elegida. El formato es:
`(Probabilidad, Estado_siguiente, Recompensa, ¬øEs_estado_terminal?)`

### 4.5 Implementaci√≥n del algoritmo de iteraci√≥n de valores

El algoritmo de iteraci√≥n de valores es un m√©todo de programaci√≥n din√°mica que calcula la funci√≥n de valor √≥ptima de manera iterativa. A partir de esta funci√≥n, podemos derivar la pol√≠tica √≥ptima.

```python
def value_iteration(env, gamma=0.99, threshold=1e-4):
    """
    Implementaci√≥n del algoritmo de iteraci√≥n de valores.
    
    Args:
        env: Entorno de Gymnasium
        gamma: Factor de descuento
        threshold: Criterio de convergencia
        
    Returns:
        V: Vector de valores √≥ptimos para cada estado
    """
    # Inicializaci√≥n
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    V = torch.zeros(n_states)
    
    # Iteraci√≥n hasta convergencia
    while True:
        # Guardar valores actuales
        V_prev = V.clone()
        
        # Actualizar valores para cada estado
        for state in range(n_states):
            # Calcular valores para cada acci√≥n
            action_values = torch.zeros(n_actions)
            
            for action in range(n_actions):
                for trans_prob, next_state, reward, _ in env.env.P[state][action]:
                    # Bellman update
                    action_values[action] += trans_prob * (reward + gamma * V_prev[next_state])
            
            # Actualizar con el m√°ximo valor
            V[state] = torch.max(action_values)
        
        # Verificar convergencia
        max_delta = torch.max(torch.abs(V - V_prev))
        if max_delta < threshold:
            break
            
    return V
```

### 4.6 Extracci√≥n de la pol√≠tica √≥ptima

Una vez que tenemos la funci√≥n de valor √≥ptima, podemos extraer la pol√≠tica √≥ptima:

```python
def extract_policy(env, V, gamma=0.99):
    """
    Extrae la pol√≠tica √≥ptima a partir de la funci√≥n de valor.
    
    Args:
        env: Entorno de Gymnasium
        V: Vector de valores √≥ptimos
        gamma: Factor de descuento
        
    Returns:
        policy: Vector de acciones √≥ptimas para cada estado
    """
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    policy = torch.zeros(n_states)
    
    for state in range(n_states):
        # Calcular valores Q para cada acci√≥n
        Q = torch.zeros(n_actions)
        
        for action in range(n_actions):
            for trans_prob, next_state, reward, _ in env.env.P[state][action]:
                Q[action] += trans_prob * (reward + gamma * V[next_state])
        
        # Seleccionar la acci√≥n con mayor valor
        policy[state] = torch.argmax(Q)
        
    return policy
```

### 4.7 Evaluaci√≥n de la pol√≠tica √≥ptima

Finalmente, evaluamos el rendimiento de la pol√≠tica √≥ptima:

```python
# Calcular valores y pol√≠tica √≥ptima
V_optimal = value_iteration(env, gamma=0.99, threshold=1e-4)
optimal_policy = extract_policy(env, V_optimal, gamma=0.99)

# Funci√≥n para evaluar la pol√≠tica
def evaluate_policy(env, policy, n_episodes=1000):
    """
    Eval√∫a el rendimiento de una pol√≠tica mediante simulaci√≥n.
    
    Args:
        env: Entorno de Gymnasium
        policy: Vector de acciones para cada estado
        n_episodes: N√∫mero de episodios a simular
        
    Returns:
        success_rate: Tasa de √©xito (porcentaje de episodios exitosos)
    """
    success_count = 0
    
    for _ in range(n_episodes):
        state, _ = env.reset()
        done = False
        
        while not done:
            action = int(policy[state].item())
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Verificar si fue exitoso (lleg√≥ a la meta)
            if done and reward == 1.0:
                success_count += 1
                
    return success_count / n_episodes

# Evaluar la pol√≠tica √≥ptima
success_rate = evaluate_policy(env, optimal_policy, n_episodes=1000)
print(f"Tasa de √©xito con pol√≠tica √≥ptima: {success_rate:.2%}")
```

Con la pol√≠tica √≥ptima, se logra una tasa de √©xito cercana al 74%, lo que representa una mejora significativa respecto al 1.6% obtenido con pol√≠ticas aleatorias.

### 4.8 Implementaci√≥n de iteraci√≥n de pol√≠ticas

Otro enfoque de programaci√≥n din√°mica es el algoritmo de iteraci√≥n de pol√≠ticas, que alterna entre evaluaci√≥n y mejora de la pol√≠tica:

```python
def policy_evaluation(env, policy, gamma=0.99, threshold=1e-4):
    """
    Eval√∫a una pol√≠tica calculando su funci√≥n de valor.
    
    Args:
        env: Entorno de Gymnasium
        policy: Vector de acciones para cada estado
        gamma: Factor de descuento
        threshold: Criterio de convergencia
        
    Returns:
        V: Vector de valores para la pol√≠tica dada
    """
    n_states = policy.shape[0]
    V = torch.zeros(n_states)
    
    while True:
        V_prev = V.clone()
        
        for state in range(n_states):
            action = int(policy[state].item())
            
            # Calcular valor esperado para la acci√≥n dada
            value = 0
            for trans_prob, next_state, reward, _ in env.env.P[state][action]:
                value += trans_prob * (reward + gamma * V_prev[next_state])
                
            V[state] = value
            
        # Verificar convergencia
        if torch.max(torch.abs(V - V_prev)) < threshold:
            break
            
    return V

def policy_improvement(env, V, gamma=0.99):
    """
    Mejora la pol√≠tica bas√°ndose en la funci√≥n de valor actual.
    
    Args:
        env: Entorno de Gymnasium
        V: Vector de valores para cada estado
        gamma: Factor de descuento
        
    Returns:
        policy: Vector de acciones mejoradas para cada estado
    """
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    policy = torch.zeros(n_states)
    
    for state in range(n_states):
        Q = torch.zeros(n_actions)
        
        for action in range(n_actions):
            for trans_prob, next_state, reward, _ in env.env.P[state][action]:
                Q[action] += trans_prob * (reward + gamma * V[next_state])
                
        policy[state] = torch.argmax(Q)
        
    return policy

def policy_iteration(env, gamma=0.99, threshold=1e-4):
    """
    Implementaci√≥n del algoritmo de iteraci√≥n de pol√≠ticas.
    
    Args:
        env: Entorno de Gymnasium
        gamma: Factor de descuento
        threshold: Criterio de convergencia
        
    Returns:
        V: Vector de valores √≥ptimos
        policy: Vector de acciones √≥ptimas
    """
    # Inicializaci√≥n con pol√≠tica aleatoria
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    policy = torch.randint(high=n_actions, size=(n_states,)).float()
    
    while True:
        # Evaluaci√≥n de pol√≠tica
        V = policy_evaluation(env, policy, gamma, threshold)
        
        # Mejora de pol√≠tica
        new_policy = policy_improvement(env, V, gamma)
        
        # Verificar convergencia
        if torch.equal(new_policy, policy):
            return V, new_policy
            
        policy = new_policy
```

## 5. Aprendizaje Monte Carlo con el Entorno Blackjack

### 5.1 M√©todos de aprendizaje sin modelo

A diferencia de los m√©todos de programaci√≥n din√°mica (value iteration, policy iteration), los m√©todos de aprendizaje por refuerzo sin modelo (model-free) no requieren conocimiento expl√≠cito de las probabilidades de transici√≥n ni de las recompensas del entorno. Estos m√©todos aprenden directamente a trav√©s de la experiencia, recopilando informaci√≥n al interactuar con el entorno.

Monte Carlo (MC) es uno de estos m√©todos, caracterizado por:

* Aprender exclusivamente de episodios completos de experiencia
* No requerir conocimiento previo del modelo del entorno
* Actualizar estimaciones bas√°ndose en retornos reales observados
* Ser especialmente eficaz en entornos con informaci√≥n parcial o estoc√°sticos

### 5.2 Descripci√≥n del entorno Blackjack

Blackjack es un juego de cartas donde el jugador compite contra el crupier. El objetivo es obtener una mano cuyo valor se acerque lo m√°s posible a 21 sin pasarse.

![Blackjack](https://gymnasium.farama.org/_images/blackjack.gif)

Caracter√≠sticas principales del entorno en Gymnasium:

* **Estado**: Tripla (suma_jugador, carta_visible_crupier, as_usable)
  - suma_jugador: Valor total de las cartas del jugador
  - carta_visible_crupier: Valor de la carta visible del crupier
  - as_usable: Booleano que indica si el jugador tiene un As que cuenta como 11

* **Acciones**:
  - 0: Plantarse (stick)
  - 1: Pedir carta (hit)

* **Recompensas**:
  - +1: Victoria del jugador
  - 0: Empate
  - -1: Victoria del crupier

* **Reglas de cartas**:
  - Cartas num√©ricas (2-10): Valor nominal
  - Figuras (J, Q, K): 10 puntos
  - As: 1 u 11 puntos (11 si no causa que se supere 21)

### 5.3 Creaci√≥n y simulaci√≥n del entorno

```python
# Crear entorno Blackjack
env = gym.make('Blackjack-v1')

# Reiniciar entorno
state, _ = env.reset(seed=42)
print(f"Estado inicial: {state}")
# Ejemplo: (14, 10, False) - Jugador tiene 14, crupier muestra 10, sin As usable

# Simular algunas acciones
print("\nSimulaci√≥n de un episodio:")
done = False
total_reward = 0

while not done:
    # Pedir carta (hit)
    state, reward, terminated, truncated, _ = env.step(1)
    print(f"Acci√≥n: Pedir carta -> Estado: {state}, Recompensa: {reward}")
    
    # Si tenemos 18 o m√°s, nos plantamos
    if state[0] >= 18 or terminated or truncated:
        if not (terminated or truncated):
            state, reward, terminated, truncated, _ = env.step(0)
            print(f"Acci√≥n: Plantarse -> Estado: {state}, Recompensa: {reward}")
        
        done = terminated or truncated
        total_reward = reward

print(f"\nRecompensa final: {total_reward}")
```

### 5.4 Evaluaci√≥n de pol√≠ticas con Monte Carlo First-Visit

El algoritmo Monte Carlo first-visit eval√∫a una pol√≠tica estimando la funci√≥n de valor de estado mediante la recopilaci√≥n de retornos promedio de la primera vez que se visita cada estado en m√∫ltiples episodios.

```python
def run_blackjack_episode(env, policy):
    """
    Ejecuta un episodio completo de Blackjack siguiendo una pol√≠tica fija.
    
    Args:
        env: Entorno Blackjack
        policy: Funci√≥n que toma un estado y devuelve una acci√≥n (0=stick, 1=hit)
        
    Returns:
        states: Lista de estados visitados
        rewards: Lista de recompensas recibidas
    """
    states = []
    rewards = []
    
    state, _ = env.reset()
    done = False
    
    while not done:
        states.append(state)
        
        # Determinar acci√≥n seg√∫n la pol√≠tica
        action = policy(state)
        
        # Ejecutar acci√≥n
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        rewards.append(reward)
        state = next_state
        
    return states, rewards

def mc_prediction_first_visit(env, policy, gamma=1.0, n_episodes=500000):
    """
    Estimaci√≥n Monte Carlo first-visit de la funci√≥n de valor de estado.
    
    Args:
        env: Entorno Blackjack
        policy: Funci√≥n que toma un estado y devuelve una acci√≥n
        gamma: Factor de descuento
        n_episodes: N√∫mero de episodios a simular
        
    Returns:
        V: Diccionario con los valores estimados para cada estado
    """
    # Inicializaci√≥n
    returns_sum = {}  # Suma de retornos para cada estado
    returns_count = {}  # Contador de visitas para cada estado
    V = {}  # Valor estimado para cada estado
    
    for i in range(n_episodes):
        # Mostrar progreso
        if i % 100000 == 0:
            print(f"Episodio {i}/{n_episodes}")
            
        # Ejecutar un episodio
        states, rewards = run_blackjack_episode(env, policy)
        
        # Calcular retornos para cada paso
        G = 0
        states.reverse()  # Invertir para procesar desde el final
        rewards.reverse()
        
        # Procesar cada paso
        for t, (state, reward) in enumerate(zip(states, rewards)):
            G = gamma * G + reward
            
            # First-visit: solo consideramos la primera vez que vemos el estado
            if state not in states[:t]:
                # Actualizar contadores
                if state not in returns_sum:
                    returns_sum[state] = 0
                    returns_count[state] = 0
                    
                returns_sum[state] += G
                returns_count[state] += 1
                
                # Actualizar estimaci√≥n del valor
                V[state] = returns_sum[state] / returns_count[state]
                
    return V
```

### 5.5 Evaluaci√≥n de una pol√≠tica simple

Vamos a evaluar una pol√≠tica simple: pedir carta (hit) hasta alcanzar al menos 18 puntos, luego plantarse (stick).

```python
# Definir pol√≠tica simple
def simple_policy(state):
    player_sum, dealer_card, usable_ace = state
    return 1 if player_sum < 18 else 0  # hit si < 18, stick en caso contrario

# Evaluar la pol√≠tica
V = mc_prediction_first_visit(env, simple_policy, gamma=1.0, n_episodes=500000)

# Verificar n√∫mero de estados evaluados
print(f"N√∫mero de estados evaluados: {len(V)}")

# Imprimir algunos valores de ejemplo
print("\nEjemplos de valores de estado:")
examples = [(13, 10, False), (19, 7, True), (18, 7, False)]
for state in examples:
    if state in V:
        print(f"Estado {state}: {V[state]:.3f}")
```

### 5.6 Visualizaci√≥n de la funci√≥n de valor

Podemos visualizar la funci√≥n de valor para entender mejor la pol√≠tica:

```python
import numpy as np

# Preparar matrices para visualizaci√≥n
player_sums = np.arange(12, 22)
dealer_cards = np.arange(1, 11)

# Matriz para As usable
value_usable = np.zeros((len(player_sums), len(dealer_cards)))
for i, player_sum in enumerate(player_sums):
    for j, dealer_card in enumerate(dealer_cards):
        state = (player_sum, dealer_card, True)
        if state in V:
            value_usable[i, j] = V[state]
        else:
            value_usable[i, j] = 0
            
# Matriz para As no usable
value_no_usable = np.zeros((len(player_sums), len(dealer_cards)))
for i, player_sum in enumerate(player_sums):
    for j, dealer_card in enumerate(dealer_cards):
        state = (player_sum, dealer_card, False)
        if state in V:
            value_no_usable[i, j] = V[state]
        else:
            value_no_usable[i, j] = 0

# Visualizaci√≥n
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Con As usable
im0 = axes[0].imshow(value_usable, cmap='viridis', extent=[1, 10, 12, 21])
axes[0].set_xlabel('Carta visible del crupier')
axes[0].set_ylabel('Suma del jugador')
axes[0].set_title('Valor con As usable')
fig.colorbar(im0, ax=axes[0])

# Sin As usable
im1 = axes[1].imshow(value_no_usable, cmap='viridis', extent=[1, 10, 12, 21])
axes[1].set_xlabel('Carta visible del crupier')
axes[1].set_ylabel('Suma del jugador')
axes[1].set_title('Valor sin As usable')
fig.colorbar(im1, ax=axes[1])

plt.tight_layout()
plt.show()
```

## 6. Control Monte Carlo On-Policy para Blackjack

### 6.1 De la evaluaci√≥n al control

Si bien la evaluaci√≥n de pol√≠ticas nos ayuda a entender qu√© tan buena es una pol√≠tica fija, el objetivo final del aprendizaje por refuerzo es encontrar la pol√≠tica √≥ptima. El **control Monte Carlo on-policy** extiende el enfoque de evaluaci√≥n para mejorar iterativamente la pol√≠tica mientras la evaluamos.

El proceso sigue un ciclo de:
1. Evaluaci√≥n de la pol√≠tica actual
2. Mejora de la pol√≠tica basada en los valores aprendidos
3. Repetici√≥n hasta convergencia

A diferencia de los m√©todos de programaci√≥n din√°mica, el control Monte Carlo on-policy aprende directamente de la experiencia sin requerir un modelo del entorno.

### 6.2 Algoritmo de control Monte Carlo on-policy

```python
def mc_control_on_policy(env, gamma=1.0, n_episodes=500000):
    """
    Control Monte Carlo on-policy para encontrar una pol√≠tica √≥ptima.
    
    Args:
        env: Entorno Blackjack
        gamma: Factor de descuento
        n_episodes: N√∫mero de episodios a simular
        
    Returns:
        Q: Diccionario con valores Q para cada par estado-acci√≥n
        policy: Pol√≠tica √≥ptima resultante
    """
    # Inicializaci√≥n
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    returns_sum = defaultdict(float)
    returns_count = defaultdict(int)
    
    # Pol√≠tica epsilon-greedy
    def epsilon_greedy_policy(state, epsilon=0.1):
        if np.random.random() < epsilon:
            return np.random.choice(env.action_space.n)  # Exploraci√≥n
        else:
            return np.argmax(Q[state])  # Explotaci√≥n
    
    for i in range(n_episodes):
        # Mostrar progreso
        if i % 100000 == 0:
            print(f"Episodio {i}/{n_episodes}")
        
        # Generar episodio
        episode = []
        state, _ = env.reset()
        done = False
        
        while not done:
            action = epsilon_greedy_policy(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            episode.append((state, action, reward))
            state = next_state
        
        # Extraer estados y acciones √∫nicas del episodio
        state_action_pairs = set([(s, a) for s, a, _ in episode])
        
        # Actualizar valores Q para cada par (estado, acci√≥n) del episodio
        for state, action, _ in episode:
            # Solo considerar pares v√°lidos (jugador no ha superado 21)
            if state[0] <= 21 and (state, action) in state_action_pairs:
                # First-visit: extraer el retorno desde este paso
                first_occurrence_idx = next(i for i, x in enumerate(episode) 
                                         if x[0] == state and x[1] == action)
                
                # Calcular retorno desde primera ocurrencia
                G = sum([gamma**i * r for i, (_, _, r) 
                       in enumerate(episode[first_occurrence_idx:])])
                
                # Actualizar estimaciones
                returns_sum[(state, action)] += G
                returns_count[(state, action)] += 1
                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]
                
                # Eliminar par para no contarlo nuevamente (first-visit)
                state_action_pairs.remove((state, action))
    
    # Extraer pol√≠tica √≥ptima final
    policy = {}
    for state in Q.keys():
        policy[state] = np.argmax(Q[state])
        
    return Q, policy
```

### 6.3 Entrenamiento y evaluaci√≥n

```python
# Entrenar el agente
Q_optimal, optimal_policy = mc_control_on_policy(env, gamma=1.0, n_episodes=500000)

# Definir funci√≥n de simulaci√≥n para cualquier pol√≠tica
def simulate_episode(env, policy):
    """
    Simula un episodio completo siguiendo una pol√≠tica dada.
    
    Args:
        env: Entorno Blackjack
        policy: Diccionario que mapea estados a acciones
        
    Returns:
        reward: Recompensa final del episodio
    """
    state, _ = env.reset()
    done = False
    
    while not done:
        # La pol√≠tica puede no tener todas las combinaciones posibles,
        # en ese caso usamos una acci√≥n por defecto (0 = stick)
        action = policy.get(state, 0)
        
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        if done:
            return reward

# Comparar pol√≠tica √≥ptima vs pol√≠tica simple
n_episodes = 100000
rewards_optimal = []
rewards_simple = []

print("Evaluando pol√≠ticas...")
for _ in range(n_episodes):
    # Evaluar pol√≠tica √≥ptima
    rewards_optimal.append(simulate_episode(env, optimal_policy))
    
    # Evaluar pol√≠tica simple (plantarse en 18)
    state, _ = env.reset()
    done = False
    while not done:
        action = 1 if state[0] < 18 else 0
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        if done:
            rewards_simple.append(reward)

# Calcular tasas de victoria
win_rate_optimal = sum(r == 1 for r in rewards_optimal) / n_episodes
win_rate_simple = sum(r == 1 for r in rewards_simple) / n_episodes

print(f"Tasa de victoria con pol√≠tica simple: {win_rate_simple:.4f}")
print(f"Tasa de victoria con pol√≠tica √≥ptima: {win_rate_optimal:.4f}")
print(f"Mejora: {(win_rate_optimal - win_rate_simple) * 100:.2f}%")
```

### 6.4 Visualizaci√≥n de la pol√≠tica √≥ptima

Para entender mejor la pol√≠tica √≥ptima, podemos visualizarla:

```python
# Preparar matrices para visualizaci√≥n
player_sums = np.arange(12, 22)
dealer_cards = np.arange(1, 11)

# Matriz para As usable
policy_usable = np.zeros((len(player_sums), len(dealer_cards)))
for i, player_sum in enumerate(player_sums):
    for j, dealer_card in enumerate(dealer_cards):
        state = (player_sum, dealer_card, True)
        if state in optimal_policy:
            policy_usable[i, j] = optimal_policy[state]
        else:
            policy_usable[i, j] = 0
            
# Matriz para As no usable
policy_no_usable = np.zeros((len(player_sums), len(dealer_cards)))
for i, player_sum in enumerate(player_sums):
    for j, dealer_card in enumerate(dealer_cards):
        state = (player_sum, dealer_card, False)
        if state in optimal_policy:
            policy_no_usable[i, j] = optimal_policy[state]
        else:
            policy_no_usable[i, j] = 0

# Visualizaci√≥n
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Con As usable
im0 = axes[0].imshow(policy_usable, cmap='Accent', extent=[1, 10, 12, 21])
axes[0].set_xlabel('Carta visible del crupier')
axes[0].set_ylabel('Suma del jugador')
axes[0].set_title('Pol√≠tica √≥ptima con As usable')
axes[0].set_xticks(np.arange(1, 11))
axes[0].set_yticks(np.arange(12, 22))
cbar0 = fig.colorbar(im0, ax=axes[0], ticks=[0, 1])
cbar0.ax.set_yticklabels(['Plantarse', 'Pedir carta'])

# Sin As usable
im1 = axes[1].imshow(policy_no_usable, cmap='Accent', extent=[1, 10, 12, 21])
axes[1].set_xlabel('Carta visible del crupier')
axes[1].set_ylabel('Suma del jugador')
axes[1].set_title('Pol√≠tica √≥ptima sin As usable')
axes[1].set_xticks(np.arange(1, 11))
axes[1].set_yticks(np.arange(12, 22))
cbar1 = fig.colorbar(im1, ax=axes[1], ticks=[0, 1])
cbar1.ax.set_yticklabels(['Plantarse', 'Pedir carta'])

plt.tight_layout()
plt.show()
```

Esta visualizaci√≥n revela patrones interesantes en la estrategia √≥ptima:
- Con sumas bajas (12-16), generalmente conviene pedir carta
- Con sumas altas (19-21), lo mejor es plantarse
- Las decisiones para sumas intermedias (17-18) dependen de la carta visible del crupier
- La estrategia var√≠a significativamente cuando se tiene un As usable

## 7. Aprendizaje por Diferencias Temporales: Q-learning

### 7.1 Limitaciones de Monte Carlo y ventajas de TD

El m√©todo de Monte Carlo tiene dos limitaciones principales:
1. Requiere esperar hasta el final de cada episodio para actualizar los valores
2. Puede tener alta varianza en las estimaciones debido a la naturaleza aleatoria de los episodios completos

El aprendizaje por diferencias temporales (TD Learning) supera estas limitaciones al:
- Actualizar estimaciones despu√©s de cada paso (sin esperar al final del episodio)
- Utilizar estimaciones existentes para reducir la varianza (bootstrapping)
- Converger generalmente m√°s r√°pido que Monte Carlo en muchos problemas

### 7.2 Q-learning: un algoritmo TD off-policy

Q-learning es un algoritmo TD que aprende la funci√≥n de valor √≥ptima Q* directamente, independientemente de la pol√≠tica que se est√© siguiendo (off-policy). Esto significa que puede aprender la pol√≠tica √≥ptima mientras explora el entorno con cualquier estrategia de exploraci√≥n.

La actualizaci√≥n clave en Q-learning se basa en la ecuaci√≥n de Bellman:

$$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \cdot \max_{a'} Q(s',a') - Q(s,a)]$$

Donde:
- $\alpha$ es la tasa de aprendizaje
- $\gamma$ es el factor de descuento
- $r$ es la recompensa inmediata
- $s'$ es el siguiente estado
- $\max_{a'} Q(s',a')$ es el valor m√°ximo posible en el siguiente estado

### 7.3 Implementaci√≥n de Q-learning para Blackjack

```python
def q_learning(env, gamma=1.0, alpha=0.01, epsilon=1.0, 
               final_epsilon=0.1, n_episodes=10000):
    """
    Implementaci√≥n de Q-learning para el entorno Blackjack.
    
    Args:
        env: Entorno Blackjack
        gamma: Factor de descuento
        alpha: Tasa de aprendizaje
        epsilon: Probabilidad inicial de exploraci√≥n
        final_epsilon: Probabilidad final de exploraci√≥n
        n_episodes: N√∫mero de episodios a simular
        
    Returns:
        Q: Diccionario con valores Q para cada par estado-acci√≥n
        policy: Pol√≠tica √≥ptima resultante
        rewards: Lista de recompensas por episodio
    """
    # Inicializaci√≥n
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    rewards_per_episode = np.zeros(n_episodes)
    
    # Decaimiento de epsilon
    epsilon_decay = (epsilon - final_epsilon) / (n_episodes / 2)
    
    for i in range(n_episodes):
        # Actualizar epsilon (decaimiento linear)
        if epsilon > final_epsilon:
            epsilon -= epsilon_decay
            
        # Iniciar episodio
        state, _ = env.reset()
        done = False
        
        while not done:
            # Pol√≠tica epsilon-greedy
            if np.random.random() < epsilon:
                action = np.random.choice(env.action_space.n)  # Exploraci√≥n
            else:
                action = np.argmax(Q[state])  # Explotaci√≥n
            
            # Ejecutar acci√≥n
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Actualizaci√≥n Q-learning
            best_next_action = np.argmax(Q[next_state])
            td_target = reward + gamma * Q[next_state][best_next_action] * (not done)
            td_error = td_target - Q[state][action]
            Q[state][action] += alpha * td_error
            
            # Transici√≥n al siguiente estado
            state = next_state
            
            # Registrar recompensa
            if done:
                rewards_per_episode[i] = reward
        
        # Mostrar progreso
        if (i+1) % 1000 == 0:
            print(f"Episodio {i+1}/{n_episodes}, Epsilon: {epsilon:.4f}")
    
    # Extraer pol√≠tica √≥ptima
    policy = {}
    for state in Q.keys():
        policy[state] = np.argmax(Q[state])
        
    return Q, policy, rewards_per_episode
```

### 7.4 Entrenamiento y evaluaci√≥n del agente Q-learning

```python
# Entrenar el agente
Q_values, q_policy, rewards = q_learning(env, 
                                         gamma=1.0, 
                                         alpha=0.01, 
                                         epsilon=1.0,
                                         final_epsilon=0.1,
                                         n_episodes=100000)

# Calcular media m√≥vil para visualizar el progreso del aprendizaje
def moving_average(data, window_size=100):
    """Calcula la media m√≥vil de los datos."""
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Visualizar curva de aprendizaje
plt.figure(figsize=(10, 6))
plt.plot(moving_average(rewards, 1000))
plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)
plt.xlabel('Episodio')
plt.ylabel('Recompensa promedio (media m√≥vil)')
plt.title('Curva de aprendizaje de Q-learning')
plt.ylim(-1.1, 1.1)
plt.grid(alpha=0.3)
plt.show()

# Evaluar la pol√≠tica aprendida
n_test_episodes = 100000
test_rewards = []

for _ in range(n_test_episodes):
    state, _ = env.reset()
    done = False
    
    while not done:
        action = q_policy.get(state, 0)  # Acci√≥n por defecto: plantarse
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        if done:
            test_rewards.append(reward)

# Calcular estad√≠sticas
win_rate = sum(r == 1 for r in test_rewards) / n_test_episodes
loss_rate = sum(r == -1 for r in test_rewards) / n_test_episodes
draw_rate = sum(r == 0 for r in test_rewards) / n_test_episodes

print(f"Resultados tras {n_test_episodes} episodios:")
print(f"  ‚Ä¢ Victorias: {win_rate:.4f}")
print(f"  ‚Ä¢ Derrotas: {loss_rate:.4f}")
print(f"  ‚Ä¢ Empates: {draw_rate:.4f}")
```

### 7.5 Comparaci√≥n entre los m√©todos estudiados

Para completar nuestro an√°lisis, comparemos los tres enfoques estudiados:

```python
# Simulamos episodios con las tres pol√≠ticas
n_eval_episodes = 100000
results = {
    'Pol√≠tica simple (plantarse en 18)': [],
    'Pol√≠tica Monte Carlo on-policy': [],
    'Pol√≠tica Q-learning': []
}

print("Evaluando las tres pol√≠ticas...")
for _ in range(n_eval_episodes):
    # Reiniciar entorno para cada episodio
    env.reset()
    
    # 1. Pol√≠tica simple
    state, _ = env.reset()
    done = False
    while not done:
        action = 1 if state[0] < 18 else 0
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        if done:
            results['Pol√≠tica simple (plantarse en 18)'].append(reward)
    
    # 2. Pol√≠tica Monte Carlo
    state, _ = env.reset()
    done = False
    while not done:
        action = optimal_policy.get(state, 0)
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        if done:
            results['Pol√≠tica Monte Carlo on-policy'].append(reward)
    
    # 3. Pol√≠tica Q-learning
    state, _ = env.reset()
    done = False
    while not done:
        action = q_policy.get(state, 0)
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        if done:
            results['Pol√≠tica Q-learning'].append(reward)

# Calcular tasas de victoria
performance = {}
for method, rewards in results.items():
    win_rate = sum(r == 1 for r in rewards) / n_eval_episodes
    performance[method] = win_rate

# Visualizar comparaci√≥n
plt.figure(figsize=(12, 6))
methods = list(performance.keys())
win_rates = [performance[method] for method in methods]

plt.bar(methods, win_rates, color=['#3498db', '#2ecc71', '#e74c3c'])
plt.ylabel('Tasa de victoria')
plt.title('Comparaci√≥n de rendimiento entre m√©todos')
plt.ylim(0, 0.5)

for i, v in enumerate(win_rates):
    plt.text(i, v + 0.01, f"{v:.4f}", ha='center')

plt.tight_layout()
plt.show()
```

---

### ¬øQu√© tan buena es la pol√≠tica final?

Simulamos 100,000 episodios:

```python
n_episode = 100000
n_win_opt = 0

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_opt += 1

print(f'Probabilidad de ganar con Q-learning: {n_win_opt / n_episode:.5f}')
```

**Resultado t√≠pico:**
`Probabilidad de ganar con Q-learning: 0.42398`

---

### Resumen de esta secci√≥n

* **Q-learning** aprende r√°pido, actualizando despu√©s de cada paso.
* No necesita conocer el entorno.
* Su pol√≠tica es cada vez mejor gracias a la exploraci√≥n epsilon-greedy.
* Aprendimos una pol√≠tica que **supera a una pol√≠tica simple** en Blackjack.

---

### Conclusi√≥n general del cap√≠tulo

**Narrador:** En este cap√≠tulo recorrimos el mundo del aprendizaje por refuerzo:

* Aprendimos los conceptos b√°sicos de **entorno, agente, estados, acciones y recompensas**.
* Exploramos dos m√©todos cl√°sicos: **programaci√≥n din√°mica (value/policy iteration)**.
* Luego pasamos a m√©todos sin modelo: **Monte Carlo y Q-learning**.
* Todo esto aplicado a entornos reales: **FrozenLake** y **Blackjack**.

Perfecto. Vamos con la **Parte 7: Resumen final del cap√≠tulo 15 ‚Äì Toma de decisiones en entornos complejos con Aprendizaje por Refuerzo**.

---

### üìò Parte 7: Resumen Final del Cap√≠tulo 15

---

#### üß† ¬øQu√© aprendimos?

Este cap√≠tulo nos gui√≥ por los fundamentos y t√©cnicas clave del **aprendizaje por refuerzo (Reinforcement Learning, RL)**, una poderosa rama del aprendizaje autom√°tico enfocada en la toma de decisiones √≥ptimas **a trav√©s de la experiencia**.

---

### üß© Elementos esenciales del aprendizaje por refuerzo

* **Agente:** toma decisiones (ej. Mario, un coche aut√≥nomo).
* **Entorno:** el mundo donde act√∫a el agente (un juego, una carretera).
* **Acciones:** lo que el agente puede hacer.
* **Estados:** la situaci√≥n actual del entorno.
* **Recompensas:** feedback num√©rico por las acciones tomadas.

> üéØ Objetivo: aprender una pol√≠tica que maximice la recompensa acumulada (returns), tomando decisiones √≥ptimas a lo largo del tiempo.

---

### ‚öñÔ∏è Dos enfoques principales en RL

| Enfoque                | ¬øQu√© aprende?                                                                          | Ejemplo conceptual                          |
| ---------------------- | -------------------------------------------------------------------------------------- | ------------------------------------------- |
| **Basado en pol√≠tica** | Aprende directamente qu√© acci√≥n tomar en cada estado (la pol√≠tica)                     | Ense√±ar a conducir una pista de carreras    |
| **Basado en valores**  | Aprende el valor esperado de cada estado y elige acciones que lleven a estados mejores | Buscar el tesoro eligiendo caminos valiosos |

---

### üßä Caso pr√°ctico 1: **FrozenLake**

Un entorno donde el agente debe caminar sobre hielo resbaloso sin caer en agujeros.

#### üîß T√©cnicas aplicadas:

* **Iteraci√≥n de valores:** calcula el valor de cada estado y deriva la pol√≠tica √≥ptima.
* **Iteraci√≥n de pol√≠ticas:** alterna entre evaluar y mejorar la pol√≠tica.

> ‚úÖ Ambos m√©todos requieren conocer el modelo del entorno (matriz de transiciones y recompensas).

---

### üÉè Caso pr√°ctico 2: **Blackjack**

Juego de cartas donde el jugador toma decisiones con informaci√≥n parcial.

#### üß™ T√©cnicas aplicadas:

1. **Monte Carlo (MC):**

   * Aprende sin conocer el entorno.
   * Usa el promedio de recompensas al final de los episodios.
   * On-policy: mejora su pol√≠tica mientras la eval√∫a.

2. **Q-learning:**

   * Tambi√©n sin modelo.
   * Actualiza los valores **despu√©s de cada paso**, no al final.
   * Off-policy: aprende la pol√≠tica √≥ptima aunque explore de forma aleatoria.

---

### üìä Comparaci√≥n de t√©cnicas

| T√©cnica          | Requiere modelo | Aprende por episodios | Actualizaci√≥n inmediata | Tipo de pol√≠tica                             |
| ---------------- | --------------- | --------------------- | ----------------------- | -------------------------------------------- |
| Value Iteration  | ‚úÖ               | ‚ùå                     | ‚úÖ                       | Determinista                                 |
| Policy Iteration | ‚úÖ               | ‚ùå                     | ‚úÖ                       | Determinista                                 |
| Monte Carlo      | ‚ùå               | ‚úÖ                     | ‚ùå                       | On-policy (explora y mejora)                 |
| Q-learning       | ‚ùå               | ‚úÖ                     | ‚úÖ                       | Off-policy (aprende √≥ptimo mientras explora) |

---

### üìà Resultados comparativos

* **Pol√≠tica simple (Blackjack ‚Äì plantarse en 18):** \~40% de victorias
* **Pol√≠tica √≥ptima con Monte Carlo:** \~43%
* **Pol√≠tica √≥ptima con Q-learning:** \~42.4%

---

### üß† Conclusi√≥n

> El aprendizaje por refuerzo nos permite crear **agentes inteligentes** que aprenden **experimentando y adapt√°ndose** sin necesidad de reglas predefinidas del entorno.

Desde m√©todos cl√°sicos hasta t√©cnicas modernas sin modelo, el RL ofrece un marco generalizable para resolver problemas de decisi√≥n complejos.

---

### üìö Ejercicios propuestos (traducidos):

1. **¬øPuedes intentar resolver el entorno FrozenLake 8x8 usando los algoritmos de iteraci√≥n de valores o iteraci√≥n de pol√≠ticas?**
2. **¬øPuedes implementar la versi√≥n de evaluaci√≥n Monte Carlo que promedia en *todas* las visitas (every-visit) en lugar de solo la primera (first-visit)?**

¬°Perfecto! Vamos a desarrollar los **dos ejercicios propuestos** en Python, paso a paso y comentados.

---

## 8. S√≠ntesis y Aplicaci√≥n Pr√°ctica

### 8.1 Implementaci√≥n pr√°ctica: FrozenLake 8x8

Para poner en pr√°ctica los conceptos aprendidos, resolveremos el entorno FrozenLake en su versi√≥n ampliada de 8x8. Esta implementaci√≥n nos permitir√° verificar la escalabilidad de los algoritmos de programaci√≥n din√°mica.

```python
import gymnasium as gym
import torch
import numpy as np

# Inicializar entorno
env = gym.make("FrozenLake8x8-v1", is_slippery=True)

def policy_evaluation(env, policy, gamma=0.99, threshold=1e-4):
    """Eval√∫a una pol√≠tica dada calculando su funci√≥n de valor."""
    n_states = policy.shape[0]
    V = torch.zeros(n_states)
    
    while True:
        V_prev = V.clone()
        
        for state in range(n_states):
            action = int(policy[state].item())
            value = 0
            
            for trans_prob, next_state, reward, _ in env.env.P[state][action]:
                value += trans_prob * (reward + gamma * V_prev[next_state])
                
            V[state] = value
            
        # Verificar convergencia
        if torch.max(torch.abs(V - V_prev)) < threshold:
            break
            
    return V

def policy_improvement(env, V, gamma=0.99):
    """Mejora la pol√≠tica bas√°ndose en la funci√≥n de valor actual."""
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    policy = torch.zeros(n_states)
    
    for state in range(n_states):
        Q = torch.zeros(n_actions)
        
        for action in range(n_actions):
            for trans_prob, next_state, reward, _ in env.env.P[state][action]:
                Q[action] += trans_prob * (reward + gamma * V[next_state])
                
        policy[state] = torch.argmax(Q)
        
    return policy

def policy_iteration(env, gamma=0.99, threshold=1e-4):
    """Implementaci√≥n completa del algoritmo de iteraci√≥n de pol√≠ticas."""
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    
    # Inicializar con pol√≠tica aleatoria
    policy = torch.randint(high=n_actions, size=(n_states,)).float()
    
    iteration = 0
    while True:
        iteration += 1
        
        # Paso 1: Evaluaci√≥n de pol√≠tica
        V = policy_evaluation(env, policy, gamma, threshold)
        
        # Paso 2: Mejora de pol√≠tica
        new_policy = policy_improvement(env, V, gamma)
        
        # Paso 3: Verificar convergencia
        if torch.equal(new_policy, policy):
            print(f"Pol√≠tica convergi√≥ despu√©s de {iteration} iteraciones")
            return V, new_policy
            
        policy = new_policy

# Obtener pol√≠tica √≥ptima
_, optimal_policy = policy_iteration(env)

# Funci√≥n para evaluar el rendimiento
def evaluate_policy(env, policy, n_episodes=1000):
    """Eval√∫a una pol√≠tica mediante simulaci√≥n de episodios."""
    success_count = 0
    
    for _ in range(n_episodes):
        state, _ = env.reset()
        done = False
        
        while not done:
            action = int(policy[state].item())
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            if done and reward == 1.0:
                success_count += 1
                
    return success_count / n_episodes

# Evaluar rendimiento
success_rate = evaluate_policy(env, optimal_policy, n_episodes=1000)
print(f"‚úì Tasa de √©xito con pol√≠tica √≥ptima en FrozenLake8x8: {success_rate:.2%}")
```
```

---

### ‚úÖ 1B. Policy Iteration en `FrozenLake8x8-v1`

```python
def policy_evaluation(env, policy, gamma, threshold):
    n_state = policy.shape[0]
    V = torch.zeros(n_state)
    while True:
        V_temp = torch.zeros(n_state)
        for state in range(n_state):
            action = int(policy[state].item())
            for trans_prob, new_state, reward, _ in env.env.P[state][action]:
                V_temp[state] += trans_prob * (reward + gamma * V[new_state])
        if torch.max(torch.abs(V - V_temp)) < threshold:
            break
        V = V_temp.clone()
    return V

def policy_improvement(env, V, gamma):
    n_state = env.observation_space.n
    n_action = env.action_space.n
    policy = torch.zeros(n_state)
    for state in range(n_state):
        q = torch.zeros(n_action)
        for action in range(n_action):
            for trans_prob, new_state, reward, _ in env.env.P[state][action]:
                q[action] += trans_prob * (reward + gamma * V[new_state])
        policy[state] = torch.argmax(q)
    return policy

def policy_iteration(env, gamma, threshold):
    n_state = env.observation_space.n
    n_action = env.action_space.n
    policy = torch.randint(high=n_action, size=(n_state,)).float()
    
    while True:
        V = policy_evaluation(env, policy, gamma, threshold)
        new_policy = policy_improvement(env, V, gamma)
        if torch.equal(new_policy, policy):
            return V, new_policy
        policy = new_policy

# Ejecutar
V_pi, policy_pi = policy_iteration(env, gamma, threshold)
print("Ejercicio 1B - Policy Iteration completado.")
```

---

### 8.2 Implementaci√≥n avanzada: Monte Carlo Every-Visit

El m√©todo Monte Carlo First-Visit solo actualiza el valor de un estado la primera vez que aparece en un episodio. La variante Every-Visit, por otro lado, actualiza el valor cada vez que se visita el estado, lo que puede proporcionar estimaciones m√°s precisas en algunos contextos.

A continuaci√≥n se presenta una implementaci√≥n del m√©todo Monte Carlo Every-Visit para el entorno Blackjack:

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

# Crear entorno Blackjack
env = gym.make("Blackjack-v1")

def run_episode(env, policy):
    """
    Ejecuta un episodio completo siguiendo una pol√≠tica determinada.
    
    Args:
        env: Entorno Blackjack
        policy: Funci√≥n que toma un estado y devuelve una acci√≥n
        
    Returns:
        states: Lista de estados visitados
        rewards: Lista de recompensas recibidas
    """
    state, _ = env.reset()
    states = [state]
    rewards = []
    done = False
    
    while not done:
        # Determinar acci√≥n seg√∫n la pol√≠tica
        action = policy(state)
        
        # Ejecutar acci√≥n
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        # Registrar estado y recompensa
        states.append(next_state)
        rewards.append(reward)
        
        # Actualizar estado
        state = next_state
        
    return states, rewards

def mc_every_visit(env, policy, gamma=1.0, n_episodes=500000):
    """
    Evaluaci√≥n Monte Carlo every-visit de la funci√≥n de valor.
    
    Args:
        env: Entorno Blackjack
        policy: Funci√≥n que toma un estado y devuelve una acci√≥n
        gamma: Factor de descuento
        n_episodes: N√∫mero de episodios a simular
        
    Returns:
        V: Diccionario con los valores estimados para cada estado
    """
    # Inicializaci√≥n
    returns_sum = defaultdict(float)
    returns_count = defaultdict(int)
    
    for i in range(n_episodes):
        # Mostrar progreso
        if i % 100000 == 0:
            print(f"Episodio {i}/{n_episodes}")
        
        # Ejecutar episodio
        states, rewards = run_episode(env, policy)
        
        # Calcular retornos
        G = 0
        for t in range(len(rewards) - 1, -1, -1):
            G = gamma * G + rewards[t]
            state = states[t]
            
            # Every-visit: actualizar cada vez que visitamos el estado
            if state[0] <= 21:  # Solo estados v√°lidos (jugador no ha perdido)
                returns_sum[state] += G
                returns_count[state] += 1
    
    # Calcular valores promedio
    V = {}
    for state in returns_sum:
        V[state] = returns_sum[state] / returns_count[state]
        
    return V

# Definir pol√≠tica simple: Pedir carta hasta sumar 18 o m√°s
def simple_policy(state):
    player_sum, dealer_card, usable_ace = state
    return 1 if player_sum < 18 else 0  # 1=hit, 0=stick

# Ejecutar el algoritmo
V = mc_every_visit(env, simple_policy, gamma=1.0, n_episodes=500000)

# Visualizar resultados
def visualize_value_function(V):
    # Preparar matrices para visualizaci√≥n
    player_sums = np.arange(12, 22)
    dealer_cards = np.arange(1, 11)
    
    # Separar por uso de As
    V_usable = np.zeros((len(player_sums), len(dealer_cards)))
    V_no_usable = np.zeros((len(player_sums), len(dealer_cards)))
    
    # Llenar matrices
    for i, player_sum in enumerate(player_sums):
        for j, dealer_card in enumerate(dealer_cards):
            # Con As usable
            if (player_sum, dealer_card, True) in V:
                V_usable[i, j] = V[(player_sum, dealer_card, True)]
            
            # Sin As usable
            if (player_sum, dealer_card, False) in V:
                V_no_usable[i, j] = V[(player_sum, dealer_card, False)]
    
    # Crear visualizaci√≥n
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Con As usable
    im0 = axes[0].imshow(V_usable, cmap='viridis', extent=[1, 10, 12, 21])
    axes[0].set_title('Funci√≥n de valor con As usable')
    axes[0].set_xlabel('Carta visible del crupier')
    axes[0].set_ylabel('Suma del jugador')
    fig.colorbar(im0, ax=axes[0])
    
    # Sin As usable
    im1 = axes[1].imshow(V_no_usable, cmap='viridis', extent=[1, 10, 12, 21])
    axes[1].set_title('Funci√≥n de valor sin As usable')
    axes[1].set_xlabel('Carta visible del crupier')
    axes[1].set_ylabel('Suma del jugador')
    fig.colorbar(im1, ax=axes[1])
    
    plt.tight_layout()
    plt.show()

# Mostrar resultados
print(f"N√∫mero de estados evaluados: {len(V)}")
visualize_value_function(V)
```
```

## 9. Resumen y Conclusiones

### 9.1 Conceptos y m√©todos fundamentales del aprendizaje por refuerzo

El aprendizaje por refuerzo representa un paradigma √∫nico dentro del aprendizaje autom√°tico, centrado en la toma de decisiones secuenciales a trav√©s de la interacci√≥n directa con el entorno. A lo largo de este cap√≠tulo hemos explorado:

1. **Fundamentos te√≥ricos**:
   - El ciclo de interacci√≥n agente-entorno
   - Formulaci√≥n matem√°tica de los procesos de decisi√≥n de Markov (MDP)
   - Funciones de valor y pol√≠ticas

2. **M√©todos de programaci√≥n din√°mica**:
   - Iteraci√≥n de valores
   - Iteraci√≥n de pol√≠ticas

3. **M√©todos libres de modelo**:
   - Monte Carlo (first-visit y every-visit)
   - Aprendizaje por diferencias temporales (Q-learning)

### 9.2 Tabla comparativa de algoritmos

| Algoritmo | Tipo | Requiere modelo | Eficiencia muestral | Actualizaci√≥n | Convergencia | Aplicabilidad |
|-----------|------|-----------------|---------------------|---------------|--------------|---------------|
| Value Iteration | Prog. din√°mica | S√≠ | Alta | Por iteraci√≥n | Garantizada | Entornos peque√±os con modelo conocido |
| Policy Iteration | Prog. din√°mica | S√≠ | Alta | Por iteraci√≥n | Garantizada | Entornos peque√±os con modelo conocido |
| Monte Carlo | Model-free | No | Baja | Al final del episodio | Asint√≥tica | Problemas epis√≥dicos |
| Q-learning | Model-free | No | Media | Por paso | Asint√≥tica | Amplio rango de problemas |

### 9.3 Resultados comparativos

En nuestras implementaciones pr√°cticas, hemos observado los siguientes resultados:

- **FrozenLake-v1 (4x4)**:
  - Pol√≠tica aleatoria: ~1.6% de tasa de √©xito
  - Pol√≠tica √≥ptima (programaci√≥n din√°mica): ~74% de tasa de √©xito

- **FrozenLake8x8-v1**:
  - Pol√≠tica √≥ptima (programaci√≥n din√°mica): ~75% de tasa de √©xito

- **Blackjack-v1**:
  - Pol√≠tica simple (plantarse en 18): ~40% de victorias
  - Pol√≠tica Monte Carlo: ~43% de victorias
  - Pol√≠tica Q-learning: ~42.4% de victorias

### 9.4 Aplicaciones y perspectivas futuras

El aprendizaje por refuerzo ha demostrado ser extraordinariamente potente en una amplia gama de aplicaciones:

- **Rob√≥tica**: Control motor y manipulaci√≥n de objetos
- **Videojuegos**: Agentes que superan el rendimiento humano (AlphaGo, AlphaStar)
- **Optimizaci√≥n de sistemas**: Gesti√≥n de recursos, redes el√©ctricas
- **Finanzas**: Trading algor√≠tmico y gesti√≥n de carteras
- **Medicina**: Dosificaci√≥n personalizada y planes de tratamiento

Las tendencias actuales apuntan hacia m√©todos m√°s escalables, que integran aprendizaje profundo con RL (Deep Reinforcement Learning), y algoritmos m√°s eficientes en t√©rminos de muestras.

### 9.5 Recursos adicionales para profundizar

Para lectores interesados en expandir su conocimiento, recomendamos:

1. **Libros**:
   - "Reinforcement Learning: An Introduction" (Sutton & Barto)
   - "Deep Reinforcement Learning Hands-On" (Maxim Lapan)

2. **Cursos en l√≠nea**:
   - CS234: Reinforcement Learning (Stanford)
   - Deep RL Bootcamp (Berkeley)

3. **Frameworks**:
   - Gymnasium (sucesor de OpenAI Gym)
   - Stable Baselines3
   - TensorFlow Agents
   - RLlib

El aprendizaje por refuerzo sigue siendo un campo en r√°pida evoluci√≥n, con nuevos algoritmos y aplicaciones emergiendo constantemente. Las bases que hemos explorado en este cap√≠tulo proporcionan un s√≥lido fundamento para comprender y contribuir a estos avances.

---

*Este cap√≠tulo fue elaborado como parte del material educativo avanzado sobre inteligencia artificial y aprendizaje autom√°tico.*

---

## ‚úÖ C√≥digo para evaluar una pol√≠tica en FrozenLake8x8

Este c√≥digo usa **policy iteration** para obtener la pol√≠tica √≥ptima y luego la **eval√∫a simulando 1,000 episodios**.

```python
import gymnasium as gym
import torch
import numpy as np

# Inicializar entorno
env = gym.make("FrozenLake8x8-v1", is_slippery=True)

# Funciones de policy iteration
def policy_evaluation(env, policy, gamma, threshold):
    n_state = policy.shape[0]
    V = torch.zeros(n_state)
    while True:
        V_temp = torch.zeros(n_state)
        for state in range(n_state):
            action = int(policy[state].item())
            for trans_prob, new_state, reward, _ in env.env.P[state][action]:
                V_temp[state] += trans_prob * (reward + gamma * V[new_state])
        if torch.max(torch.abs(V - V_temp)) < threshold:
            break
        V = V_temp.clone()
    return V

def policy_improvement(env, V, gamma):
    n_state = env.observation_space.n
    n_action = env.action_space.n
    policy = torch.zeros(n_state)
    for state in range(n_state):
        q = torch.zeros(n_action)
        for action in range(n_action):
            for trans_prob, new_state, reward, _ in env.env.P[state][action]:
                q[action] += trans_prob * (reward + gamma * V[new_state])
        policy[state] = torch.argmax(q)
    return policy

def policy_iteration(env, gamma=0.99, threshold=1e-4):
    n_state = env.observation_space.n
    n_action = env.action_space.n
    policy = torch.randint(high=n_action, size=(n_state,)).float()
    while True:
        V = policy_evaluation(env, policy, gamma, threshold)
        new_policy = policy_improvement(env, V, gamma)
        if torch.equal(new_policy, policy):
            return V, new_policy
        policy = new_policy

# Obtener pol√≠tica √≥ptima
_, optimal_policy = policy_iteration(env)

# Simular pol√≠tica √≥ptima
def evaluate_policy(env, policy, n_episodes=1000):
    success_count = 0
    for _ in range(n_episodes):
        state, _ = env.reset()
        done = False
        while not done:
            action = int(policy[state].item())
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            if done and reward == 1.0:
                success_count += 1
    return success_count / n_episodes

# Evaluar rendimiento
success_rate = evaluate_policy(env, optimal_policy, n_episodes=1000)
print(f"‚úîÔ∏è Tasa de √©xito con pol√≠tica √≥ptima en FrozenLake8x8: {success_rate:.2%}")
```

---

Este c√≥digo deber√≠a darte un resultado como:

```
‚úîÔ∏è Tasa de √©xito con pol√≠tica √≥ptima en FrozenLake8x8: 75.3%
```
